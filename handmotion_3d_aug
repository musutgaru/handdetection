import os
import numpy as np

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Masking, LSTM, Dense, Dropout, BatchNormalization
)
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ProgbarLogger
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.optimizers.schedules import ExponentialDecay

# (live_quiz_session에서만 사용할 경우)
from tensorflow.keras.applications import VGG16

from tensorflow.keras.utils import to_categorical


param = {
    'batch_size':           32,
    'time_steps':           60,    # 이미 60프레임으로 저장된 데이터를 그대로 씁니다
    'height':               224,
    'width':                224,
    'channels':             3,
    'lstm_dropout':         0.2,
    'lstm_recurrent_dropout': 0.2,
    'model_dense_units1':   64,    # → 256에서 64로 축소
    'model_dropout1':       0.3,
    'learning_rate':        1e-3,
    'patience':             5,
    'model_dir':            'best_sign_lang_model_3d.h5',
    'epoch':                500,
    'rnd_seed':             42
}

def rotate_keypoints(seq, angle_deg):
    theta = np.deg2rad(angle_deg)
    R = np.array([[np.cos(theta), -np.sin(theta)],
                  [np.sin(theta),  np.cos(theta)]], dtype=np.float32)
    idx_xy = [(i, i+1) for i in range(0, seq.shape[1], 3)]
    seq_aug = seq.copy()
    for t in range(seq_aug.shape[0]):
        pts = np.array([[seq_aug[t, i], seq_aug[t, j]] for i, j in idx_xy], dtype=np.float32)
        center = np.array([0.5, 0.5], dtype=np.float32)
        pts_norm = pts - center
        pts_rot  = pts_norm.dot(R.T) + center
        for k, (i, j) in enumerate(idx_xy):
            seq_aug[t, i] = pts_rot[k, 0]
            seq_aug[t, j] = pts_rot[k, 1]
    return seq_aug

def scale_keypoints(seq, s):
    idx_xy = [(i, i+1) for i in range(0, seq.shape[1], 3)]
    seq_aug = seq.copy()
    for t in range(seq_aug.shape[0]):
        for i, j in idx_xy:
            seq_aug[t, i] = (seq_aug[t, i] - 0.5) * s + 0.5
            seq_aug[t, j] = (seq_aug[t, j] - 0.5) * s + 0.5
    return seq_aug

def translate_keypoints(seq, tx, ty):
    idx_xy = [(i, i+1) for i in range(0, seq.shape[1], 3)]
    seq_aug = seq.copy()
    for t in range(seq_aug.shape[0]):
        for i, j in idx_xy:
            seq_aug[t, i] += tx
            seq_aug[t, j] += ty
    return seq_aug

def augment_keypoints(per_seq):
    ang = np.random.uniform(-30, 30)
    s   = np.random.uniform(0.8, 1.2)
    tx  = np.random.uniform(-0.05, 0.05)
    ty  = np.random.uniform(-0.05, 0.05)
    out = rotate_keypoints(per_seq, ang)
    out = scale_keypoints(out, s)
    out = translate_keypoints(out, tx, ty)
    return out

  # ─── Cell 3 (3D 키포인트 추출 및 padding) ───────────────────────────────────────
import os, shutil, cv2, numpy as np, pandas as pd, mediapipe as mp
from tensorflow.keras.utils import to_categorical

def extract_preprocess_and_save(
    video_dir, frames_dir, excel_path,
    fps=param['time_steps'], size=(param['height'], param['width']),
    augment_factor=5
):
    """
    1) MediaPipe로 3D 키포인트(138차원)를 추출
    2) 양 어깨 기준 2D 정규화
    3) 원본 + 랜덤 프레임 샘플링 기반 오프라인 증강(augment_factor 배)
    4) 기하학적 augmentation(회전, 스케일, 평행이동)
    5) per-video .npz 백업 및 누적 X_keypoints.npy, y_onehot.npy 저장
    """
    # 0) 이어쓰기 묻기
    npy_x = os.path.join(frames_dir, 'X_keypoints_3d_extra.npy')
    npy_y = os.path.join(frames_dir, 'y_onehot_3d_extra.npy')
    resume = os.path.exists(npy_x) and input(f"'{frames_dir}' 이어쓰기? ([y]/n): ").strip().lower() in ('','y','yes')

    # 1) 초기화 or 유지
    if not resume:
        shutil.rmtree(frames_dir, ignore_errors=True)
        os.makedirs(frames_dir, exist_ok=True)
        for f in (npy_x, npy_y):
            try: os.remove(f)
            except: pass
    else:
        os.makedirs(frames_dir, exist_ok=True)

    # 2) 메타 & 클래스 맵
    meta = pd.read_excel(excel_path, engine='openpyxl')
    meta['한국어'] = meta['한국어'].astype(str).str.strip()
    meta['파일명'] = meta['파일명'].astype(str).str.strip()
    classes = sorted(meta['한국어'].unique())
    c2i     = {c:i for i,c in enumerate(classes)}
    num_cls = len(classes)

    # 3) NPY 로드 or 초기화
    if resume and os.path.exists(npy_x):
        X_all = np.load(npy_x)   # (N_prev, fps, 138)
        y_all = np.load(npy_y)
    else:
        X_all = np.zeros((0, fps, 138), dtype=np.float32)
        y_all = np.zeros((0, num_cls), dtype=np.float32)

    # 4) MediaPipe 세팅
    mp_pose  = mp.solutions.pose.Pose(min_detection_confidence=0.5,
                                      min_tracking_confidence=0.5)
    mp_hands = mp.solutions.hands.Hands(min_detection_confidence=0.5,
                                        min_tracking_confidence=0.5,
                                        max_num_hands=2)

    # 5) 비디오별 처리
    for _, row in meta.iterrows():
        name, _ = os.path.splitext(row['파일명'])
        lbl_idx = c2i[row['한국어']]

        # (5.1) 비디오 파일 찾기
        vid = None
        for ext in ('.MTS', '.MOV', '.MP4', '.AVI'):
            candidate = os.path.join(video_dir, name + ext)
            if os.path.isfile(candidate):
                vid = candidate
                break
        if vid is None:
            print("경로 없음:", name)
            continue

        # (5.2) 프레임 읽기 & 균등 샘플링 인덱스
        cap = cv2.VideoCapture(vid)
        frames = []
        while True:
            ret, frm = cap.read()
            if not ret: break
            frames.append(frm)
        cap.release()
        M = len(frames)
        if M == 0:
            print(f"[{row['한국어']}/{name}] 프레임 없음, 스킵")
            continue

        # (5.3) 원본 + 증강 시퀀스 생성
        for k in range(augment_factor):
            if k == 0:
                idxs = np.linspace(0, M-1, fps, dtype=int)
            else:
                idxs = np.sort(np.random.choice(np.arange(M), size=fps, replace=True))

            seq = []
            for i in idxs:
                frame = cv2.resize(frames[i], size)
                rgb   = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                p_res = mp_pose.process(rgb)
                h_res = mp_hands.process(rgb)

                # (5.x) 3D 키포인트 수집
                coords = []
                # 어깨·팔꿈치 4점 → (x,y,z)
                if p_res.pose_landmarks:
                    for j in (11,13,12,14):
                        lm = p_res.pose_landmarks.landmark[j]
                        coords += [lm.x, lm.y, lm.z]
                else:
                    coords += [0.0]*12
                # 손 21점씩 최대 2손 → (x,y,z)
                hands = (h_res.multi_hand_landmarks or [])[:2]
                for hand_lm in hands:
                    for lm in hand_lm.landmark:
                        coords += [lm.x, lm.y, lm.z]
                # 패딩
                if len(hands) == 0:
                    coords += [0.0]*126
                elif len(hands) == 1:
                    coords += [0.0]*63

                # (5.y) 2D 객체 기준 정규화 (양 어깨 중심 기준 이동+스케일)
                cx = (coords[0] + coords[6]) / 2
                cy = (coords[1] + coords[7]) / 2
                scale = np.hypot(coords[6]-coords[0], coords[7]-coords[1]) + 1e-6
                for t in range(0, len(coords), 3):
                    coords[t]   = (coords[t]   - cx) / scale
                    coords[t+1] = (coords[t+1] - cy) / scale
                    # z는 변형하지 않음

                seq.append(coords)

            per_seq = np.array(seq, dtype=np.float32)  # (fps,138)

            # (5.z) 기하학적 keypoint augmentation (회전, 스케일, 이동)
            if k > 0:
                per_seq = augment_keypoints(per_seq)

            per_lbl = to_categorical(lbl_idx, num_cls)
            suffix = f"_aug{k}" if k>0 else ""

            print(f"[{row['한국어']}/{name}]{suffix} 완료: {len(seq)}/{fps}")

            # (5.4) per-video .npz 백업
            np.savez(os.path.join(frames_dir, f"{name}{suffix}.npz"),
                     pts=per_seq, label=per_lbl)

            # 누적 NPY에 붙이기
            X_all = np.concatenate([X_all, per_seq[np.newaxis,...]], axis=0)
            y_all = np.concatenate([y_all, per_lbl[np.newaxis,...]], axis=0)

        # 5.5) 누적 저장
        np.save(npy_x, X_all)
        np.save(npy_y, y_all)

    # 6) 클래스 리스트 저장
    np.save(os.path.join(frames_dir, 'class_list.npy'),
            np.array(classes, dtype=object))

  def load_VGG16():
    base_model = VGG16(
        weights='imagenet',
        include_top=False,
        input_shape=(param['height'], param['width'], param['channels'])
    )
    base_model.trainable = True
    return base_model

import tensorflow as tf
import cv2

def build_tfdata_dataset(frames_root, excel_path, param):
    # … 이전과 동일 …
    return ds, class_list

  from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Masking, LSTM, BatchNormalization, Dense, Dropout
from tensorflow.keras.optimizers import Adam

def sign_lang_model():
    # 두 개의 LSTM 블록, 각 128 유닛
    model = Sequential([
        # 138차원 입력을 받도록 변경
        Masking(mask_value=0.0, input_shape=(param['time_steps'], 138)),

        # 1st LSTM, 시퀀스 전체를 다음 레이어에 넘기기
        LSTM(
            128,
            activation='relu',
            dropout=param['lstm_dropout'],
            recurrent_dropout=param['lstm_recurrent_dropout'],
            return_sequences=True
        ),
        BatchNormalization(),

        # 2nd LSTM, 마지막 타임스텝만 출력
        LSTM(
            128,
            activation='relu',
            dropout=param['lstm_dropout'],
            recurrent_dropout=param['lstm_recurrent_dropout']
        ),
        BatchNormalization(),

        # 은닉층: 256 유닛으로 증강
        Dense(256, activation='relu'),
        Dropout(param['dropout']),

        # 최종 출력: 클래스 수
        Dense(param['num_classes'], activation='softmax')
    ])

    # Adam 옵티마이저로 변경, learning rate는 param 활용
    model.compile(
        optimizer=Adam(learning_rate=param['learning_rate']),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

def train_model(model, dataset_train, dataset_val):
    # 1) Early stopping on val_accuracy
    es = EarlyStopping(
        monitor='val_accuracy',
        patience=param['patience'],
        restore_best_weights=True,
        verbose=1
    )
    # 2) Save best model by val_accuracy
    ckpt = ModelCheckpoint(
        filepath=param['model_dir'],
        monitor='val_accuracy',
        save_best_only=True,
        verbose=1
    )
    # 3) Reduce LR on plateau of val_loss
    rlrop = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=3,
        min_lr=1e-6,
        verbose=1
    )

    callbacks = [es, ckpt, rlrop]

    # 4) fit: dataset already batched, so no batch_size arg
    history = model.fit(
        dataset_train,
        validation_data=dataset_val,
        epochs=param['epochs'],
        callbacks=callbacks,
        shuffle=True,
        verbose=1
    )

    print(f"Stopped at epoch {es.stopped_epoch + 1}, best val_accuracy: {max(history.history['val_accuracy']):.4f}")
    return history, es.stopped_epoch


  
  import os

video_root = os.path.expanduser("~/Desktop/jeongeun/DL/수어 데이터셋")
frames_root = os.path.expanduser("~/Desktop/jeongeun/DL/keypoints") # keypoint로 바꿈
if os.path.exists(frames_root):
    shutil.rmtree(frames_root)
excel_path  = os.path.join(video_root,
               'KETI-2017-SL-Annotation-v2_1.xlsx')

extract_preprocess_and_save(
    video_dir=video_root,
    frames_dir=frames_root,
    excel_path=excel_path,
    fps=param['time_steps'],
    size=(param['height'], param['width'])
)

import os
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split

def build_keypoint_dataset(frames_dir: str, param: dict):
    # 1) npy 로드
    x_all = np.load(os.path.join(frames_dir, 'X_keypoints_3d_extra.npy'))  # (N, T, D)
    y_all = np.load(os.path.join(frames_dir, 'y_onehot_3d_extra.npy'))     # (N, C)

    # 2) train/val 분할 (optional: 이미 Cell 3에서 분할하셨다면 생략)
    X_train, X_val, y_train, y_val = train_test_split(
        x_all, y_all,
        test_size=0.2,
        stratify=y_all.argmax(axis=1),
        random_state=param.get('rnd_seed', 42)
    )

    # 3) tf.data.Dataset 생성
    def make_ds(X, y, shuffle=True):
        ds = tf.data.Dataset.from_tensor_slices((X, y))
        if shuffle:
            ds = ds.shuffle(buffer_size=len(X), seed=param.get('rnd_seed',42))
        ds = ds.batch(param['batch_size'])
        return ds.prefetch(tf.data.AUTOTUNE)

    return make_ds(X_train, y_train), make_ds(X_val, y_val, shuffle=False)

import matplotlib.pyplot as plt

def plot_loss_acc(history, upto_epoch=None):
    fig, (axL, axA) = plt.subplots(1, 2, figsize=(12,4))

    ep = history.history['loss']
    val_ep = history.history['val_loss']
    axL.plot(ep,     label='train loss')
    axL.plot(val_ep, label='val   loss')
    axL.set_title('Loss')
    axL.legend()

    acc = history.history['accuracy']
    vacc = history.history['val_accuracy']
    axA.plot(acc,  label='train acc')
    axA.plot(vacc, label='val   acc')
    axA.set_title('Accuracy')
    axA.legend()

    if upto_epoch is not None:
        axL.axvline(upto_epoch-1, color='gray', linestyle='--')
        axA.axvline(upto_epoch-1, color='gray', linestyle='--')

    plt.show()


  
  from sklearn.model_selection import train_test_split
import tensorflow as tf

# 1) NPY 불러오기
X = np.load(os.path.join(frames_root, 'X_keypoints_3d_extra.npy'))   # (N,60,92)
y = np.load(os.path.join(frames_root, 'y_onehot_3d_extra.npy'))      # (N,419)

# 2) train/val 분할 (stratify)
y_idx = y.argmax(axis=1)
X_train, X_val, y_train, y_val = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y_idx,
    random_state=param['rnd_seed']
)
print("Train:", X_train.shape, y_train.shape)
print("Valid:", X_val.shape,   y_val.shape)

# 3) 래핑
train_ds = (
    tf.data.Dataset.from_tensor_slices((X_train, y_train))
    .shuffle(buffer_size=len(X_train))
    .batch(param['batch_size'], drop_remainder=True)
    .prefetch(tf.data.AUTOTUNE)
)
val_ds = (
    tf.data.Dataset.from_tensor_slices((X_val, y_val))
    .batch(param['batch_size'], drop_remainder=False)
    .prefetch(tf.data.AUTOTUNE)
)

# 4) 모델 생성 & 학습
model = sign_lang_model()
history, stopped = train_model(model, train_ds, val_ds)

# 5) 그래프
plot_loss_acc(history, upto_epoch=stopped+1)


