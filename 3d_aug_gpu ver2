import os
import cv2
import glob
import shutil
import numpy as np
import pandas as pd
import torch
import random
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
import mediapipe as mp
from tensorflow.keras.utils import to_categorical  # 한 번만 최상단에

# Device 설정 (PyTorch GPU만)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Using device:", device)

# 파라미터
param = {
    'video_dir':        os.path.expanduser('~/Desktop/jeongeun/DL/수어 데이터셋'),
    'excel_path':       os.path.expanduser('~/Desktop/jeongeun/DL/수어 데이터셋/KETI-2017-SL-Annotation-v2_1.xlsx'),
    'out_dir':          os.path.expanduser('~/Desktop/jeongeun/DL/keypoints_np_v2'),  # ← 여기를 바꿨습니다
    'time_steps':       30,    # ← 프레임을 절반(30)으로
    'augment_factor':   20,
    'batch_size':       16,
    'epochs':           100,
    'learning_rate':    1e-3,
    'patience':         5,
    'rnd_seed':         42,
    'height':           224,
    'width':            224,
}
random.seed(param['rnd_seed'])
np.random.seed(param['rnd_seed'])



import numpy as np

def rotate_keypoints(seq, angle_deg):
    theta = np.deg2rad(angle_deg)
    R = np.array([[np.cos(theta), -np.sin(theta)],
                  [np.sin(theta),  np.cos(theta)]], dtype=np.float32)
    out = seq.copy()
    idx_xy = [(i, i+1) for i in range(0, seq.shape[1], 3)]
    center = np.array([0.5, 0.5], dtype=np.float32)
    for t in range(out.shape[0]):
        pts = np.stack([out[t, i:i+2] for i,j in idx_xy])
        pts = (pts - center) @ R.T + center
        for k,(i,j) in enumerate(idx_xy):
            out[t,i], out[t,j] = pts[k]
    return out

def scale_keypoints(seq, s):
    out = seq.copy()
    for (i,j) in [(i,i+1) for i in range(0, seq.shape[1], 3)]:
        out[:,i] = (out[:,i]-0.5)*s + 0.5
        out[:,j] = (out[:,j]-0.5)*s + 0.5
    return out

def translate_keypoints(seq, tx, ty):
    out = seq.copy()
    for (i,j) in [(i,i+1) for i in range(0, seq.shape[1], 3)]:
        out[:,i] += tx
        out[:,j] += ty
    return out

def augment_keypoints(seq):
    ang = np.random.uniform(-30,30)
    s   = np.random.uniform(0.8,1.2)
    tx  = np.random.uniform(-0.05,0.05)
    ty  = np.random.uniform(-0.05,0.05)
    out = rotate_keypoints(seq, ang)
    out = scale_keypoints(out, s)
    out = translate_keypoints(out, tx, ty)
    return out




import os
import cv2
import shutil
import numpy as np
import pandas as pd
import mediapipe as mp

def extract_and_save_keypoints(param, selected_classes):
    video_dir  = param['video_dir']
    excel_path = param['excel_path']
    out_dir    = param['out_dir']
    fps        = param['time_steps']
    augf       = param['augment_factor']
    H, W       = param['height'], param['width']

    # 1) 출력 디렉토 초기화
    if os.path.exists(out_dir):
        shutil.rmtree(out_dir)
    os.makedirs(out_dir, exist_ok=True)

    # 2) 메타 로드 & 클래스 필터
    meta = pd.read_excel(excel_path, engine='openpyxl')
    meta['한국어'] = meta['한국어'].str.strip()
    meta = meta[meta['한국어'].isin(selected_classes)].reset_index(drop=True)
    class2idx = {c:i for i,c in enumerate(selected_classes)}

    # 3) MediaPipe 세팅
    mp_pose  = mp.solutions.pose.Pose(min_detection_confidence=0.5,
                                      min_tracking_confidence=0.5)
    mp_hands = mp.solutions.hands.Hands(min_detection_confidence=0.5,
                                        min_tracking_confidence=0.5,
                                        max_num_hands=2)

    all_X, all_y = [], []

    # 4) 영상별 처리
    for _, row in meta.iterrows():
        name    = os.path.splitext(row['파일명'])[0]
        lbl_idx = class2idx[row['한국어']]

        # 영상 찾기
        path = None
        for ext in ('.MP4','.MOV','.AVI','.MTS'):
            cand = os.path.join(video_dir, name+ext)
            if os.path.isfile(cand):
                path = cand; break
        if path is None:
            print("못 찾음:", name); continue

        cap = cv2.VideoCapture(path)
        frames = []
        while True:
            ret, frm = cap.read()
            if not ret: break
            frames.append(frm)
        cap.release()
        M = len(frames)
        if M == 0:
            print(f"[{name}] 프레임 없음, 스킵"); continue

        # 5) augment_factor 배
        for k in range(augf):
            if k == 0:
                idxs = np.linspace(0, M-1, fps, dtype=int)
            else:
                idxs = np.random.choice(M, fps, replace=True)

            seq = []
            for i in idxs:
                img = cv2.resize(frames[i], (W,H))
                rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                p_res = mp_pose.process(rgb)
                h_res = mp_hands.process(rgb)

                coords = []
                if p_res.pose_landmarks:
                    for j in (11,13,12,14):
                        lm = p_res.pose_landmarks.landmark[j]
                        coords += [lm.x, lm.y, lm.z]
                else:
                    coords += [0.0]*12
                hands = (h_res.multi_hand_landmarks or [])[:2]
                for hand in hands:
                    for lm in hand.landmark:
                        coords += [lm.x, lm.y, lm.z]
                if len(hands) == 0:
                    coords += [0.0]*126
                elif len(hands) == 1:
                    coords += [0.0]*63

                # 어깨 중심 정규화
                cx = (coords[0]+coords[6]) * 0.5
                cy = (coords[1]+coords[7]) * 0.5
                sc = np.hypot(coords[6]-coords[0], coords[7]-coords[1]) + 1e-6
                for t in range(0, len(coords), 3):
                    coords[t]   = (coords[t]-cx)/sc
                    coords[t+1] = (coords[t+1]-cy)/sc

                seq.append(coords)

            arr = np.array(seq, dtype=np.float32)  # (fps,138)
            if k > 0:
                arr = augment_keypoints(arr)

            all_X.append(arr)
            all_y.append(to_categorical(lbl_idx, len(selected_classes)))

            print(f"[{row['한국어']}/{name}] aug{k} 완료")

    X = np.stack(all_X, axis=0)  # (N,30,138)
    y = np.stack(all_y, axis=0)  # (N,5)
    np.save(os.path.join(out_dir,'X.npy'), X)
    np.save(os.path.join(out_dir,'y.npy'), y)
    np.save(os.path.join(out_dir,'classes.npy'),
            np.array(selected_classes, object))
    print("저장 완료:", X.shape, y.shape)





# 5개 클래스 무작위 샘플링
meta_all = pd.read_excel(param['excel_path'], engine='openpyxl')
cls_all  = meta_all['한국어'].str.strip().unique().tolist()
random.shuffle(cls_all)
selected_5 = cls_all[:5]
print("Selected classes:", selected_5)

# 키포인트 추출 + 저장 (버전 2)
extract_and_save_keypoints(param, selected_5)



import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np

class KeypointDataset(Dataset):
    def __init__(self, X, y):
        self.X = X
        self.y = y.argmax(axis=1)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        x = torch.from_numpy(self.X[idx]).float()  # (T,138)
        y = torch.tensor(self.y[idx], dtype=torch.long)
        return x.to(device), y.to(device)

# NPY 로드
data_dir = param['out_dir']
X = np.load(os.path.join(data_dir,'X.npy'))
y = np.load(os.path.join(data_dir,'y.npy'))

# train/val split
idxs = np.arange(len(X))
tr, va = train_test_split(idxs,
                          test_size=0.2,
                          stratify=y.argmax(axis=1),
                          random_state=param['rnd_seed'])

train_ds = KeypointDataset(X[tr], y[tr])
val_ds   = KeypointDataset(X[va], y[va])

train_loader = DataLoader(train_ds,
    batch_size=param['batch_size'], shuffle=True,
    num_workers=0, pin_memory=False
)
val_loader   = DataLoader(val_ds,
    batch_size=param['batch_size'], shuffle=False,
    num_workers=0, pin_memory=False
)

print("Train batches:", len(train_loader), "Val batches:", len(val_loader))




import torch.nn as nn

class KPNet(nn.Module):
    def __init__(self, time_steps, feat_dim, n_classes, dropout=0.3):
        super().__init__()
        self.lstm1 = nn.LSTM(input_size=feat_dim, hidden_size=128,
                             batch_first=True)
        self.bn1   = nn.BatchNorm1d(128)
        self.lstm2 = nn.LSTM(input_size=128, hidden_size=128,
                             batch_first=True)
        self.fc1   = nn.Linear(128, 64)
        self.drop  = nn.Dropout(dropout)
        self.out   = nn.Linear(64, n_classes)

    def forward(self, x):
        o,_ = self.lstm1(x)        # (B,T,128)
        o = o[:,-1,:]              # (B,128)
        o = self.bn1(o)
        o = torch.relu(o)
        o,_ = self.lstm2(o.unsqueeze(1))  # (B,1,128)
        o = o[:, -1, :]
        o = torch.relu(self.fc1(o))
        o = self.drop(o)
        return self.out(o)

# 모델 생성
model = KPNet(param['time_steps'], 138, len(selected_5),
              dropout=0.3).to(device)
print(model)




import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast
from tqdm import tqdm

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=param['learning_rate'])
scheduler = optim.lr_scheduler.OneCycleLR(
    optimizer,
    max_lr=param['learning_rate'],
    steps_per_epoch=len(train_loader),
    epochs=param['epochs'],
)
scaler = GradScaler()

best_val = 0.0
no_imp   = 0

for epoch in range(1, param['epochs']+1):
    # ─── Training ─────────────────────
    model.train()
    run_loss = 0.0
    pbar = tqdm(train_loader, desc=f"[Epoch {epoch}] Train")
    for x, y in pbar:
        optimizer.zero_grad()
        with autocast():
            logits = model(x)           # mixed precision
            loss   = criterion(logits, y)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        scheduler.step()

        run_loss += loss.item() * x.size(0)
        pbar.set_postfix(loss=run_loss/((pbar.n+1)*param['batch_size']))

    train_loss = run_loss / len(train_loader.dataset)

    # ─── Validation ────────────────────
    model.eval()
    correct = 0
    with torch.no_grad():
        for x, y in tqdm(val_loader, desc=f"[Epoch {epoch}] Val"):
            preds = model(x).argmax(dim=1)
            correct += (preds == y).sum().item()
    val_acc = correct / len(val_loader.dataset)

    print(f"→ Epoch {epoch}: train_loss={train_loss:.4f}, val_acc={val_acc:.4f}")

    # ─── Early Stopping ────────────────
    if val_acc > best_val:
        best_val = val_acc
        no_imp   = 0
        torch.save(model.state_dict(),
                   os.path.join(data_dir,'best_kpnet_amp_v2.pt'))
        print("  🎉 New best, model saved.")
    else:
        no_imp += 1
        if no_imp >= param['patience']:
            print(f"☕ Early stop at epoch {epoch}")
            break

