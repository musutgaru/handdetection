import os
import cv2
import glob
import shutil
import numpy as np
import pandas as pd
import torch
import random
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
import mediapipe as mp
from tensorflow.keras.utils import to_categorical  # í•œ ë²ˆë§Œ ìµœìƒë‹¨ì—

# Device ì„¤ì • (PyTorch GPUë§Œ)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Using device:", device)

# íŒŒë¼ë¯¸í„°
param = {
    'video_dir':        os.path.expanduser('~/Desktop/jeongeun/DL/ìˆ˜ì–´ ë°ì´í„°ì…‹'),
    'excel_path':       os.path.expanduser('~/Desktop/jeongeun/DL/ìˆ˜ì–´ ë°ì´í„°ì…‹/KETI-2017-SL-Annotation-v2_1.xlsx'),
    'out_dir':          os.path.expanduser('~/Desktop/jeongeun/DL/keypoints_np_v2'),  # â† ì—¬ê¸°ë¥¼ ë°”ê¿¨ìŠµë‹ˆë‹¤
    'time_steps':       30,    # â† í”„ë ˆìž„ì„ ì ˆë°˜(30)ìœ¼ë¡œ
    'augment_factor':   20,
    'batch_size':       16,
    'epochs':           100,
    'learning_rate':    1e-3,
    'patience':         5,
    'rnd_seed':         42,
    'height':           224,
    'width':            224,
}
random.seed(param['rnd_seed'])
np.random.seed(param['rnd_seed'])



import numpy as np

def rotate_keypoints(seq, angle_deg):
    theta = np.deg2rad(angle_deg)
    R = np.array([[np.cos(theta), -np.sin(theta)],
                  [np.sin(theta),  np.cos(theta)]], dtype=np.float32)
    out = seq.copy()
    idx_xy = [(i, i+1) for i in range(0, seq.shape[1], 3)]
    center = np.array([0.5, 0.5], dtype=np.float32)
    for t in range(out.shape[0]):
        pts = np.stack([out[t, i:i+2] for i,j in idx_xy])
        pts = (pts - center) @ R.T + center
        for k,(i,j) in enumerate(idx_xy):
            out[t,i], out[t,j] = pts[k]
    return out

def scale_keypoints(seq, s):
    out = seq.copy()
    for (i,j) in [(i,i+1) for i in range(0, seq.shape[1], 3)]:
        out[:,i] = (out[:,i]-0.5)*s + 0.5
        out[:,j] = (out[:,j]-0.5)*s + 0.5
    return out

def translate_keypoints(seq, tx, ty):
    out = seq.copy()
    for (i,j) in [(i,i+1) for i in range(0, seq.shape[1], 3)]:
        out[:,i] += tx
        out[:,j] += ty
    return out

def augment_keypoints(seq):
    ang = np.random.uniform(-30,30)
    s   = np.random.uniform(0.8,1.2)
    tx  = np.random.uniform(-0.05,0.05)
    ty  = np.random.uniform(-0.05,0.05)
    out = rotate_keypoints(seq, ang)
    out = scale_keypoints(out, s)
    out = translate_keypoints(out, tx, ty)
    return out




import os
import cv2
import shutil
import numpy as np
import pandas as pd
import mediapipe as mp

def extract_and_save_keypoints(param, selected_classes):
    video_dir  = param['video_dir']
    excel_path = param['excel_path']
    out_dir    = param['out_dir']
    fps        = param['time_steps']
    augf       = param['augment_factor']
    H, W       = param['height'], param['width']

    # 1) ì¶œë ¥ ë””ë ‰í†  ì´ˆê¸°í™”
    if os.path.exists(out_dir):
        shutil.rmtree(out_dir)
    os.makedirs(out_dir, exist_ok=True)

    # 2) ë©”íƒ€ ë¡œë“œ & í´ëž˜ìŠ¤ í•„í„°
    meta = pd.read_excel(excel_path, engine='openpyxl')
    meta['í•œêµ­ì–´'] = meta['í•œêµ­ì–´'].str.strip()
    meta = meta[meta['í•œêµ­ì–´'].isin(selected_classes)].reset_index(drop=True)
    class2idx = {c:i for i,c in enumerate(selected_classes)}

    # 3) MediaPipe ì„¸íŒ…
    mp_pose  = mp.solutions.pose.Pose(min_detection_confidence=0.5,
                                      min_tracking_confidence=0.5)
    mp_hands = mp.solutions.hands.Hands(min_detection_confidence=0.5,
                                        min_tracking_confidence=0.5,
                                        max_num_hands=2)

    all_X, all_y = [], []

    # 4) ì˜ìƒë³„ ì²˜ë¦¬
    for _, row in meta.iterrows():
        name    = os.path.splitext(row['íŒŒì¼ëª…'])[0]
        lbl_idx = class2idx[row['í•œêµ­ì–´']]

        # ì˜ìƒ ì°¾ê¸°
        path = None
        for ext in ('.MP4','.MOV','.AVI','.MTS'):
            cand = os.path.join(video_dir, name+ext)
            if os.path.isfile(cand):
                path = cand; break
        if path is None:
            print("ëª» ì°¾ìŒ:", name); continue

        cap = cv2.VideoCapture(path)
        frames = []
        while True:
            ret, frm = cap.read()
            if not ret: break
            frames.append(frm)
        cap.release()
        M = len(frames)
        if M == 0:
            print(f"[{name}] í”„ë ˆìž„ ì—†ìŒ, ìŠ¤í‚µ"); continue

        # 5) augment_factor ë°°
        for k in range(augf):
            if k == 0:
                idxs = np.linspace(0, M-1, fps, dtype=int)
            else:
                idxs = np.random.choice(M, fps, replace=True)

            seq = []
            for i in idxs:
                img = cv2.resize(frames[i], (W,H))
                rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                p_res = mp_pose.process(rgb)
                h_res = mp_hands.process(rgb)

                coords = []
                if p_res.pose_landmarks:
                    for j in (11,13,12,14):
                        lm = p_res.pose_landmarks.landmark[j]
                        coords += [lm.x, lm.y, lm.z]
                else:
                    coords += [0.0]*12
                hands = (h_res.multi_hand_landmarks or [])[:2]
                for hand in hands:
                    for lm in hand.landmark:
                        coords += [lm.x, lm.y, lm.z]
                if len(hands) == 0:
                    coords += [0.0]*126
                elif len(hands) == 1:
                    coords += [0.0]*63

                # ì–´ê¹¨ ì¤‘ì‹¬ ì •ê·œí™”
                cx = (coords[0]+coords[6]) * 0.5
                cy = (coords[1]+coords[7]) * 0.5
                sc = np.hypot(coords[6]-coords[0], coords[7]-coords[1]) + 1e-6
                for t in range(0, len(coords), 3):
                    coords[t]   = (coords[t]-cx)/sc
                    coords[t+1] = (coords[t+1]-cy)/sc

                seq.append(coords)

            arr = np.array(seq, dtype=np.float32)  # (fps,138)
            if k > 0:
                arr = augment_keypoints(arr)

            all_X.append(arr)
            all_y.append(to_categorical(lbl_idx, len(selected_classes)))

            print(f"[{row['í•œêµ­ì–´']}/{name}] aug{k} ì™„ë£Œ")

    X = np.stack(all_X, axis=0)  # (N,30,138)
    y = np.stack(all_y, axis=0)  # (N,5)
    np.save(os.path.join(out_dir,'X.npy'), X)
    np.save(os.path.join(out_dir,'y.npy'), y)
    np.save(os.path.join(out_dir,'classes.npy'),
            np.array(selected_classes, object))
    print("ì €ìž¥ ì™„ë£Œ:", X.shape, y.shape)





# 5ê°œ í´ëž˜ìŠ¤ ë¬´ìž‘ìœ„ ìƒ˜í”Œë§
meta_all = pd.read_excel(param['excel_path'], engine='openpyxl')
cls_all  = meta_all['í•œêµ­ì–´'].str.strip().unique().tolist()
random.shuffle(cls_all)
selected_5 = cls_all[:5]
print("Selected classes:", selected_5)

# í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ + ì €ìž¥ (ë²„ì „ 2)
extract_and_save_keypoints(param, selected_5)



import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np

class KeypointDataset(Dataset):
    def __init__(self, X, y):
        self.X = X
        self.y = y.argmax(axis=1)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        x = torch.from_numpy(self.X[idx]).float()  # (T,138)
        y = torch.tensor(self.y[idx], dtype=torch.long)
        return x.to(device), y.to(device)

# NPY ë¡œë“œ
data_dir = param['out_dir']
X = np.load(os.path.join(data_dir,'X.npy'))
y = np.load(os.path.join(data_dir,'y.npy'))

# train/val split
idxs = np.arange(len(X))
tr, va = train_test_split(idxs,
                          test_size=0.2,
                          stratify=y.argmax(axis=1),
                          random_state=param['rnd_seed'])

train_ds = KeypointDataset(X[tr], y[tr])
val_ds   = KeypointDataset(X[va], y[va])

train_loader = DataLoader(train_ds,
    batch_size=param['batch_size'], shuffle=True,
    num_workers=0, pin_memory=False
)
val_loader   = DataLoader(val_ds,
    batch_size=param['batch_size'], shuffle=False,
    num_workers=0, pin_memory=False
)

print("Train batches:", len(train_loader), "Val batches:", len(val_loader))




import torch.nn as nn

class KPNet(nn.Module):
    def __init__(self, time_steps, feat_dim, n_classes, dropout=0.3):
        super().__init__()
        self.lstm1 = nn.LSTM(input_size=feat_dim, hidden_size=128,
                             batch_first=True)
        self.bn1   = nn.BatchNorm1d(128)
        self.lstm2 = nn.LSTM(input_size=128, hidden_size=128,
                             batch_first=True)
        self.fc1   = nn.Linear(128, 64)
        self.drop  = nn.Dropout(dropout)
        self.out   = nn.Linear(64, n_classes)

    def forward(self, x):
        o,_ = self.lstm1(x)        # (B,T,128)
        o = o[:,-1,:]              # (B,128)
        o = self.bn1(o)
        o = torch.relu(o)
        o,_ = self.lstm2(o.unsqueeze(1))  # (B,1,128)
        o = o[:, -1, :]
        o = torch.relu(self.fc1(o))
        o = self.drop(o)
        return self.out(o)

# ëª¨ë¸ ìƒì„±
model = KPNet(param['time_steps'], 138, len(selected_5),
              dropout=0.3).to(device)
print(model)




import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast
from tqdm import tqdm

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=param['learning_rate'])
scheduler = optim.lr_scheduler.OneCycleLR(
    optimizer,
    max_lr=param['learning_rate'],
    steps_per_epoch=len(train_loader),
    epochs=param['epochs'],
)
scaler = GradScaler()

best_val = 0.0
no_imp   = 0

for epoch in range(1, param['epochs']+1):
    # â”€â”€â”€ Training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    model.train()
    run_loss = 0.0
    pbar = tqdm(train_loader, desc=f"[Epoch {epoch}] Train")
    for x, y in pbar:
        optimizer.zero_grad()
        with autocast():
            logits = model(x)           # mixed precision
            loss   = criterion(logits, y)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        scheduler.step()

        run_loss += loss.item() * x.size(0)
        pbar.set_postfix(loss=run_loss/((pbar.n+1)*param['batch_size']))

    train_loss = run_loss / len(train_loader.dataset)

    # â”€â”€â”€ Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    model.eval()
    correct = 0
    with torch.no_grad():
        for x, y in tqdm(val_loader, desc=f"[Epoch {epoch}] Val"):
            preds = model(x).argmax(dim=1)
            correct += (preds == y).sum().item()
    val_acc = correct / len(val_loader.dataset)

    print(f"â†’ Epoch {epoch}: train_loss={train_loss:.4f}, val_acc={val_acc:.4f}")

    # â”€â”€â”€ Early Stopping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if val_acc > best_val:
        best_val = val_acc
        no_imp   = 0
        torch.save(model.state_dict(),
                   os.path.join(data_dir,'best_kpnet_amp_v2.pt'))
        print("  ðŸŽ‰ New best, model saved.")
    else:
        no_imp += 1
        if no_imp >= param['patience']:
            print(f"â˜• Early stop at epoch {epoch}")
            break

